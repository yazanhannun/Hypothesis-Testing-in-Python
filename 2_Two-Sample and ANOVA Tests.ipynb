{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Two-Sample and ANOVA Tests"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Performing t-tests"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b>1.1 Hypothesis testing workflow</b>\n",
    "\n",
    "You've seen the hypothesis testing workflow for the one sample case where you compared a sample mean to a hypothesized value, and the two sample case where you compared two sample means. In both cases, the workflow shares common steps.\n",
    "\n",
    "Place the hypothesis testing workflow steps in order from first to last.\n",
    "\n",
    "![Alt text](Hypothesis%20testing%20workflow.png)\n",
    "\n",
    "Regardless of the type of hypothesis test you are performing, it will have a workflow that follows this format."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b>1.2 Two sample mean test statistic</b>\n",
    "\n",
    "The hypothesis test for determining if there is a difference between the means of two populations uses a different type of test statistic to the z-scores you saw in Chapter 1. It's called \"t\", and it can be calculated from three values from each sample using this equation.\n",
    "\n",
    " \n",
    "$$\n",
    "  t = \\frac{\\bar{x}_{no}-\\bar{x}_{yes}}{\\sqrt{\\frac{s_{no}^2}{n_{no}}+\\frac{s_{yes}^2}{n_{yes}}}}\n",
    "$$\n",
    " \n",
    "\n",
    "While trying to determine why some shipments are late, you may wonder if the weight of the shipments that were on time is less than the weight of the shipments that were late. The late_shipments dataset has been split into a \"yes\" group, where late == \"Yes\" and a \"no\" group where late == \"No\". The weight of the shipment is given in the weight_kilograms variable.\n",
    "\n",
    "The sample means for the two groups are available as xbar_no and xbar_yes. The sample standard deviations are s_no and s_yes. The sample sizes are n_no and n_yes. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-2.3936661778766433\n"
     ]
    }
   ],
   "source": [
    "# Import the late_shipment dataframe\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "late_shipments = pd.read_feather(\"C:\\\\Users\\\\yazan\\\\Desktop\\\\Data_Analytics\\\\9-Introduction to Hypothesis Testing\\\\Datasets\\\\late_shipments.feather\")\n",
    "xbar = late_shipments.groupby('late_delivery')['weight_kilograms'].mean()\n",
    "s = late_shipments.groupby('late_delivery')['weight_kilograms'].std()\n",
    "n = late_shipments.groupby('late_delivery')['weight_kilograms'].count()\n",
    "\n",
    "xbar_no = xbar[0]\n",
    "xbar_yes = xbar[1]\n",
    "\n",
    "s_no = s[0]\n",
    "s_yes = s[1]\n",
    "\n",
    "n_no = n[0]\n",
    "n_yes = n[1]\n",
    "\n",
    "# Calculate the numerator of the test statistic\n",
    "numerator = xbar_no - xbar_yes\n",
    "\n",
    "# Calculate the denominator of the test statistic\n",
    "denominator = np.sqrt((s_no**2/n_no) + (s_yes**2/n_yes))\n",
    "\n",
    "# Calculate the test statistic\n",
    "t_stat = numerator/denominator\n",
    "\n",
    "# Print the test statistic\n",
    "print(t_stat)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When testing for differences between means, the test statistic is called 't' rather than 'z', and can be calculated using six numbers from the samples. Here, the value is about -2.39 or 2.39, depending on the order you calculated the numerator."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Calculating p-values from t-statistics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b>2.1 Why is t needed?</b>\n",
    "\n",
    "The process for calculating p-values is to start with the sample statistic, standardize it to get a test statistic, then transform it via a cumulative distribution function. In Chapter 1, that final transformation was denoted , and the CDF transformation used the (standard normal) z-distribution. In this chapter, the test statistic was denoted , and the transformation used the t-distribution.\n",
    "\n",
    "In which hypothesis testing scenario is a t-distribution needed instead of the z-distribution?\n",
    "\n",
    "- The t-distribution is just another name for the z-distribution, so they can be used interchangeably.\n",
    "\n",
    "- The t-distribution is the same thing as the z-distribution for very small sample sizes.\n",
    "\n",
    "- <b><font color='green'>When a sample standard deviation is used in estimating a standard error.</font></b>\n",
    "\n",
    "- When you are comparing the means of three or more samples, rather than comparing a single sample mean to a value.\n",
    "\n",
    "Using a sample standard deviation to estimate the standard error is computationally easier than using bootstrapping. However, to correct for the approximation, you need to use a t-distribution when transforming the test statistic to get the p-value."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b>2.2 The t-distribution</b>\n",
    "\n",
    "The t-distribution is used to calculate the p-value from the <i>t</i> test statistic, and having a sense of how the PDF and CDF look can help you understand this calculation. It has two parameters: the degrees of freedom, and the non-centrality parameter.\n",
    "\n",
    "The plots show the PDF and CDF for a t-distribution (solid black line), and for comparison show a normal distribution with the same mean and variance (gray dotted line).\n",
    "\n",
    "Which statement about the the t-distribution is true?\n",
    "\n",
    "- Like the normal distribution, the PDF of a central t-distribution is always symmetric.\n",
    "\n",
    "- As you increase the degrees of freedom, the tails of the t-distribution get fatter.\n",
    "\n",
    "- <b><font color='green'>As you increase the degrees of freedom, the t-distribution PDF and CDF curves get closer to those of a normal distribution.</font></b>\n",
    "\n",
    "- As you increase the non-centrality, the t-distribution PDF and CDF curves get closer to those of a normal distribution.\n",
    "\n",
    "The normal distribution is essentially a t-distribution with infinite degrees of freedom."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.8 64-bit (microsoft store)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "cd85abab4e42ed52d78993b1b54e037968b74145d44eb01d9e116517c8fc42a2"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
